---
typora-root-url: ../picture
---

**业务**





**解决缓存穿透**

场景：

1、恶意攻击，猜测你的key命名方式，然后估计使用一个你缓存中不会有的key进行访问。



2、第一次数据访问，这时缓存中还没有数据，则并发场景下，所有的请求都会压到数据库。



3、数据库的数据也是空，这样即使访问了数据库，也是获取不到数据，那么缓存中肯定也没有对应的数据。这样也会导致穿透。





缓存中没有数据，大量请求打到数据库，造成cpu 连接数、内存飙升 可能导致数据库崩溃



1. 缓存无效数据一段时间、如果多个请求请求一个无效key ，则可使用 singlefilght 抑制到数据库的请求。
2. 使用布隆过滤器：适用场景 有效数据量少，但是所有用户都要读的情况。 布隆过滤器性质：要是不在一定不在，要是在的话不一定在。将有效数据全部加载到布隆过滤器中，业务读的时候不在就直接返回如果有就读数据库。 接受一定的穿透。

**解决缓存雪崩**

缓存大量失效，或者缓存崩溃。 请求打到数据库最后数据库崩溃



解决办法：

优化方法：保持缓存层的高可用、使用降级限流组件



1. 将key 的过期时间均匀设置，避免统一时刻所有key 全部过期
2. 使用互斥锁，比如 singleflight 抑制缓存穿透，重建key
3. 异步重建key ，这个key 内容附带一个过期时间，请求key 的时候发现过期了，在通过上锁等方式异步更新 key，其他线程直接返回即可。











**分布式事物了解吗**

1.两阶段提交，2PC 引入一个事物协调者，准备阶段询问资源是否执行成功，全成功后及进入提交阶段。 问题，单点故障，数据可能会在提交阶段因网络问题造成部分提交。 同步阻塞，所有资源都会阻塞住





问题其实就出在每个参与者自身的状态只有自己和协调者知道，因此新协调者无法通过在场的参与者的状态推断出挂了的参与者是什么情况。





2.3PC 多了一个 pre_commit 阶段

从维基百科上看，3PC 的引入是为了解决提交阶段 2PC 协调者和某参与者都挂了之后新选举的协调者不知道当前应该提交还是回滚的问题。



3pc 和 2pc 都是数据库级别的分布式事物协议。





2.补偿事物 TCC (try confirm cancel) 核心思想是每个操作都要注册一个对应的确认和补偿操作，分为三个阶段 1. try 主要是资源检测以及预留 2.confirm 对业务系统做确认提交。 3. cancel 业务执行错误的时候进行回滚操作，释放资源。 缺点最终一致，补偿逻辑多



3.本地消息表 ：

本地消息表其实就是利用了 各系统本地的事务来实现分布式事务。

本地消息表顾名思义就是会有一张存放本地消息的表，一般都是放在数据库中，然后在执行业务的时候 将业务的执行和将消息放入消息表中的操作放在同一个事务中，这样就能保证消息放入本地表中业务肯定是执行成功的。

然后再去调用下一个操作，如果下一个操作调用成功了好说，消息表的消息状态可以直接改成已成功。

如果调用失败也没事，会有 后台任务定时去读取本地消息表，筛选出还未成功的消息再调用对应的服务，服务更新成功了再变更消息的状态。

这时候有可能消息对应的操作不成功，因此也需要重试，重试就得保证对应服务的方法是幂等的，而且一般重试会有最大次数，超过最大次数可以记录下报警让人工处理。

可以看到本地消息表其实实现的是最终一致性，容忍了数据暂时不一致的情况。



1. rockmq 支持事物，但是不知特别的了解。





**常见负载均衡算法**

轮询（Round Robin）法

随机（Random）法

加权轮询（Weight Round Robin）法

加权随机（Weight Random）法

源地址哈希（Hash）法

最小连接数（Least Connections）法







**如何排查 负载 高**

load 高代表排队的进程多，

但是 cpu 密集 、io 和内存负载高都有可能造成 总体负载高

需要逐个排查

1. cpu 直接看top 看 进程 cpu 时间和 内存 都能看
2. io  iostat -c 1 10 命令 看系统io 状况
3. 内存 vmstat 命令 看内存状况



**秒杀场景如何不超卖**

https://osjobs.net/system/posts/spike-system/



1.系统特点

大流量以及流量倾斜，大流量会集中在少量的几种商品中

2.核心问题

秒杀系统需要保证：

- 高可用：服务器不因为大流量而崩溃，同时秒杀业务不影响其他业务
- 一致性：商品不出现超卖和少卖
- 高扩展：架构适合水平扩展，在特殊活动前能够迅速扩容

基本架构：

1.锁机制：

悲观锁：优点：安全 缺点：性能低

乐观锁： 优点：并发量高 缺点：事物需要经常回滚重试

分布式锁：优点：功能分离 容易横向扩扩展 缺点：实现复杂、性能也不高



消息队列+锁：优点：对业务进行解耦，可以支持其他系统获取秒杀信息 缺点：需要保证幂等性，同一订单只能消费一次 、需要保证消息队列的可靠性，以及崩溃之后的恢复机制。



问题：消息队列方案有什么问题？

1.单个热点商品分区压力可能造成单个分区特别大，造成吞吐量或对其他业务造成影响

2.消费快但是数据库扛不住压力

解决办法：

1.前期压力测试

2.资源隔离：使用单独的消息队列，做好限流策略

3.合并请求 or 事物（很难做）



3.优化思路：

- 流量限制：

1.解决同一时间大量请求相同页面资源的问题 ：使用CDN 缓存

2.限流：

​	1.前端限流：验证答题，防止重复点击按钮

​	2.后端限流：使用限流算法只接受 10 *K 个请求后，就停止接受该商品的所有请求。同时由于有人支付超时等异常，需要系统通知限流器重新接收请求

​	3.负载均衡（这个都有吧。。。）

1. 1. 前后端安全验证：uid 符合条件验证，客户端版本安全验证，秒杀下单接口动态生成



问题：流量过大如何处理：

1.功能降级，非核心功能暂停使用

2.拒绝服务，随机丢弃请求

3.减少重试次数，防止流量雪崩





- 资源隔离：

1.识别热门商品，动态分配机器资源

​	1.提前预约

​	2.实时数据统计，找出热门商品



2.热门商品单独分库分表

问题：

秒杀商品分库分表后可能导致某个分表库存为 0 ，但其他部分还有库存，如何解决？？

1.当前分表查不到去其他库重试，缺点是流量扩散

2.通过上层路由组件记录每个库表的 库存情况，转发到其他有库存的表中

3.使用分布式缓存记录每个分表的库粗情况，每次下单先更新缓存，后续在刷新到数据库， 缺点是缓存不一致















**
 分布式事物**



https://zhuanlan.zhihu.com/p/78599954



阿里的 seata 框架

Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。



**TC (Transaction Coordinator) -** 事务协调者

维护全局和分支事务的状态，驱动全局事务提交或回滚。

**TM (Transaction Manager) -** 事务管理器

定义全局事务的范围：开始全局事务、提交或回滚全局事务。

**RM (Resource Manager) -** 资源管理器

管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。



**两阶段提交**



**TCC: try confirm cancel** 



![TM](/TM.png)



 **2PC**两阶段提交协议

两阶段提交协议：事务管理器分两个阶段来协调资源管理器，第一阶段准备资源，也就是预留事务所需的资源，如果每个资源管理器都资源预留成功，则进行第二阶段资源提交，否则协调资源管理器回滚资源。

2PC协议的核心是，划分出了事务参与者和协调者的角色，并将整个过程划分成两个阶段。

第一阶段：所有事务参与者，执行后进行预提交；直到协调者收到所有参与者的预提交才会进入第二步；

如果在协调者的超时时间内，有任意参与者的预提交preCommit没发送或未到达，都会结束事务。

第二阶段：所有事务预提交了各自的结果后，由协调者决定最终事务是成功(commit)还是失败(rollback)。





**海量数据问题**

https://mp.weixin.qq.com/s/o7jW5jeuQzQfP5JGxhO-ww

https://github.com/weitingyuk/LeetCode-Notes-Waiting/blob/main/2021-02-17/TopK.md

https://github.com/weitingyuk/LeetCode-Notes-Waiting/blob/main/2021-02-18/MergeBigFile.md

TOP K 小顶堆  https://blog.csdn.net/suibianshen2012/article/details/51905780


 两个文件包含无序的数字，数字的大小范围是**0-500w**左右。如何求两个文件中的重复的数据？
 https://github.com/weitingyuk/LeetCode-Notes-Waiting/blob/main/2021-02-18/MergeDuplicate.md





**动态是如何实现的**

依赖关系服务： 有 2000 的上限



读扩散模型



表结构：

feed_id\uid \feed_type\content



评论：

comment_id , comtent_type ， entity_id (动态id or 评论 id or 人),uid , content





**如何实现朋友圈：**

1.feed 表 

feed_id, feed_type,content ,location

2.time_line 表

Uid,feed_id,is_own，create_time

 3.评论表

comment_id,comment_type, entity_id,content



写扩散

向time_line 表里插入 n 条数据 is_own 标识是谁发布的